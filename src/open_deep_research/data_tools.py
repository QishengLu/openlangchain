import json
from datetime import datetime
from pathlib import Path
from typing import Union, List, Optional

from langchain_core.tools import tool

TOKEN_LIMIT = 5000


def _import_duckdb():
    """Import duckdb with helpful error message."""
    try:
        import duckdb
        return duckdb
    except ImportError:
        raise ImportError(
            "duckdb is required. Install it with: pip install duckdb"
        )


def _serialize_datetime(obj):
    """Convert datetime objects to ISO format strings for JSON serialization"""
    if isinstance(obj, datetime):
        return obj.isoformat()
    elif isinstance(obj, dict):
        return {key: _serialize_datetime(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [_serialize_datetime(item) for item in obj]
    else:
        return obj


def _estimate_token_count(text: str) -> int:
    """Estimate token count using character-based approximation.
    
    Approximate for Chinese/English mixed text.
    Average: 3 characters per token.
    """
    average_chars_per_token = 3
    return (len(text) + average_chars_per_token - 1) // average_chars_per_token


def _enforce_token_limit(payload: str, context: str) -> str:
    """Ensure payload stays within the token budget before returning"""
    token_estimate = _estimate_token_count(payload)
    if token_estimate <= TOKEN_LIMIT:
        return payload

    current_size = len(json.loads(payload)) if payload.startswith("[") else None
    suggested_limit = None
    if current_size:
        ratio = TOKEN_LIMIT / token_estimate
        suggested_limit = max(1, int(current_size * ratio * 0.8))  # 80% safety margin

    suggestion_parts = [
        "The query result is too large. Please adjust your query:",
        "  • Reduce the LIMIT value" + (f" (try LIMIT {suggested_limit})" if suggested_limit else ""),
        "  • Filter rows with WHERE clauses to reduce result size",
        "  • Select only necessary columns instead of SELECT *",
        "  • Use aggregation (COUNT, SUM, AVG) instead of retrieving raw rows",
    ]

    warning = {
        "error": "Result exceeds token budget",
        "context": context,
        "estimated_tokens": token_estimate,
        "token_limit": TOKEN_LIMIT,
        "rows_returned": current_size,
        "suggested_limit": suggested_limit,
        "suggestion": "\n".join(suggestion_parts),
    }
    return json.dumps(warning, ensure_ascii=False, indent=2)


def _validate_parquet_files(parquet_files: Union[str, List[str]]) -> List[str]:
    """Validate parquet files exist and return as list."""
    if isinstance(parquet_files, str):
        parquet_files = [parquet_files]

    for file_path in parquet_files:
        if not Path(file_path).exists():
            raise FileNotFoundError(
                f"Parquet file not found: {file_path}\n"
                f"Please check the file path and ensure the file exists. "
                f"You may use 'list_tables_in_directory' to discover available parquet files."
            )
    return parquet_files


@tool
def list_tables_in_directory(directory: str) -> str:
    """
    List all parquet files in a directory with metadata.
    
    Args:
        directory: Directory path to search for parquet files
        
    Returns:
        JSON string containing list of files with metadata
    """
    duckdb = _import_duckdb()
    
    dir_path = Path(directory)
    if not dir_path.exists():
        return json.dumps({"error": f"Directory not found: {directory}"})

    if not dir_path.is_dir():
        return json.dumps({"error": f"Path is not a directory: {directory}"})

    files_info = []
    cwd = Path.cwd()
    
    # Search recursively for parquet files
    parquet_files = list(dir_path.glob("**/*.parquet"))
    
    if not parquet_files:
        return json.dumps({"message": "No parquet files found in directory", "files": []})

    for file_path in parquet_files:
        file_path_str = str(file_path)
        
        try:
            conn = duckdb.connect(":memory:")
            # Use try-except block for duckdb operations
            try:
                row_count_result = conn.execute(f"SELECT COUNT(*) FROM read_parquet('{file_path_str}')").fetchone()
                row_count = row_count_result[0] if row_count_result else 0
                
                result = conn.execute(f"SELECT * FROM read_parquet('{file_path_str}') LIMIT 0")
                column_count = len(result.description)
            except Exception as e:
                # Skip files that can't be read or log error
                files_info.append({
                    "filename": file_path.name,
                    "path": str(file_path),
                    "error": f"Could not read file: {str(e)}"
                })
                continue
            finally:
                conn.close()

            # Try to make path relative to CWD for cleaner output
            display_path = file_path_str
            try:
                if file_path.is_absolute():
                    display_path = str(file_path.relative_to(cwd))
            except ValueError:
                pass

            files_info.append(
                {
                    "filename": file_path.name,
                    "path": display_path,
                    "row_count": row_count,
                    "column_count": column_count,
                }
            )
        except Exception:
            continue

    return json.dumps(files_info, ensure_ascii=False, indent=2)


@tool
def get_schema(parquet_file: str) -> str:
    """
    Get schema information of a parquet file.
    
    Args:
        parquet_file: Path to parquet file to inspect
        
    Returns:
        JSON string containing file metadata and schema
    """
    duckdb = _import_duckdb()
    
    if not Path(parquet_file).exists():
        return json.dumps({"error": f"Parquet file not found: {parquet_file}"})

    conn = duckdb.connect(":memory:")
    try:
        cwd = Path.cwd()
        parquet_file_obj = Path(parquet_file)
        
        # Try to make path relative for display
        display_path = str(parquet_file_obj)
        try:
            if parquet_file_obj.is_absolute():
                display_path = str(parquet_file_obj.relative_to(cwd))
        except ValueError:
            pass

        result = conn.execute(f"SELECT * FROM read_parquet('{parquet_file}') LIMIT 0")
        schema = [{"name": desc[0], "type": str(desc[1])} for desc in result.description]

        row_count_result = conn.execute(f"SELECT COUNT(*) FROM read_parquet('{parquet_file}')").fetchone()
        row_count = row_count_result[0] if row_count_result else 0

        schema_info = {
            "file": display_path,
            "row_count": row_count,
            "columns": schema,
        }

        result_json = json.dumps(schema_info, ensure_ascii=False, indent=2)
        return _enforce_token_limit(result_json, "get_schema")

    except Exception as e:
        return json.dumps({"error": str(e)})
    finally:
        conn.close()


@tool
def query_parquet_files(parquet_files: Union[str, List[str]], query: str, limit: int = 10) -> str:
    """
    Query parquet files using SQL syntax for data analysis and exploration.
    
    Args:
        parquet_files: Path(s) to parquet file(s). Can be a single string or list of strings.
        query: SQL query to execute. Use table names based on filenames (e.g. 'data.parquet' -> 'data').
        limit: Maximum number of records to return (default 10)
        
    Returns:
        JSON string of query results
    """
    duckdb = _import_duckdb()
    
    try:
        parquet_files = _validate_parquet_files(parquet_files)
    except FileNotFoundError as e:
        return json.dumps({"error": str(e)})

    conn = duckdb.connect(":memory:")
    table_names: set = set()

    try:
        # Register parquet files as views using filename as table name
        for file_path in parquet_files:
            base_name = Path(file_path).stem
            # Sanitize table name (simple version)
            table_name = base_name.replace("-", "_").replace(" ", "_")
            
            # Handle duplicate names
            original_table_name = table_name
            counter = 1
            while table_name in table_names:
                table_name = f"{original_table_name}_{counter}"
                counter += 1
            
            table_names.add(table_name)
            conn.execute(f"CREATE VIEW {table_name} AS SELECT * FROM read_parquet('{file_path}')")

        # Execute query
        result = conn.execute(query).fetchall()
        
        if not conn.description:
            return json.dumps({"message": "Query executed successfully but returned no results."})
            
        columns = [desc[0] for desc in conn.description]

        # Convert to list of dictionaries and serialize datetime
        rows = [dict(zip(columns, row, strict=False)) for row in result]
        serialized_rows = _serialize_datetime(rows)

        # Apply limit if specified
        if len(serialized_rows) > limit:
            serialized_rows = serialized_rows[:limit]

        result_json = json.dumps(serialized_rows, ensure_ascii=False, indent=2)
        return _enforce_token_limit(result_json, "query_parquet_files")

    except Exception as e:
        error_msg = str(e)
        # Provide contextual error messages
        if "syntax error" in error_msg.lower() or "parser error" in error_msg.lower():
            return json.dumps({
                "error": "SQL syntax error",
                "details": error_msg,
                "query": query,
                "available_tables": list(table_names)
            })
        elif "catalog" in error_msg.lower() or "table" in error_msg.lower():
            return json.dumps({
                "error": "Table reference error",
                "details": error_msg,
                "query": query,
                "available_tables": list(table_names)
            })
        else:
            return json.dumps({
                "error": "Query execution failed",
                "details": error_msg
            })
    finally:
        conn.close()
